{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:29:37.399384Z",
     "start_time": "2025-05-07T19:29:37.395626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "#\n",
    "import importlib\n",
    "import utilities.lstm_utils as lstm_utils"
   ],
   "id": "9be531e52190f129",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:29:37.414032Z",
     "start_time": "2025-05-07T19:29:37.411829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "6853a9962a424a1c",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preparing Data",
   "id": "fe8516df25855518"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:29:37.506111Z",
     "start_time": "2025-05-07T19:29:37.463808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_time_series = pd.read_csv('../../../data/df_monthly_returns_complete_percentage.csv', index_col='Date')\n",
    "\n",
    "df_time_series = df_time_series.loc[:, ~df_time_series.columns.str.contains('^Unnamed')]"
   ],
   "id": "4513018354bbc541",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:29:37.511507Z",
     "start_time": "2025-05-07T19:29:37.508806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# \n",
    "df_time_series_plus1 = df_time_series\n",
    "df_time_series = df_time_series - 1"
   ],
   "id": "b8263a875e7a6a1c",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Normalisation",
   "id": "c64a95d1ae4bdac4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:29:37.535975Z",
     "start_time": "2025-05-07T19:29:37.532940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' \n",
    "df_ts_torch = torch.from_numpy(df_time_series.values)\n",
    "# Reshape to (num_samples, num_features) for normalization\n",
    "df_ts_flat = df_ts_torch.view(-1, df_ts_torch.shape[-1])  # Shape: (1000*300, 5)\n",
    "\n",
    "# Calculate min and max per feature\n",
    "df_min = df_ts_flat.min(dim=0, keepdim=True)[0]\n",
    "df_max = df_ts_flat.max(dim=0, keepdim=True)[0]\n",
    "\n",
    "# Apply Min-Max normalization\n",
    "df_ts_normalised = (df_ts_flat - df_min) / (df_max - df_min)\n",
    "\n",
    "# Reshape back to original shape\n",
    "df_time_series_torch = df_ts_normalised.view(df_ts_torch.shape)\n",
    "'''\n"
   ],
   "id": "78b25301531f46d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\ndf_ts_torch = torch.from_numpy(df_time_series.values)\\n# Reshape to (num_samples, num_features) for normalization\\ndf_ts_flat = df_ts_torch.view(-1, df_ts_torch.shape[-1])  # Shape: (1000*300, 5)\\n\\n# Calculate min and max per feature\\ndf_min = df_ts_flat.min(dim=0, keepdim=True)[0]\\ndf_max = df_ts_flat.max(dim=0, keepdim=True)[0]\\n\\n# Apply Min-Max normalization\\ndf_ts_normalised = (df_ts_flat - df_min) / (df_max - df_min)\\n\\n# Reshape back to original shape\\ndf_time_series_torch = df_ts_normalised.view(df_ts_torch.shape)\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LSTM Model",
   "id": "d5b44de5e3c4223c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:29:37.562338Z",
     "start_time": "2025-05-07T19:29:37.559007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=1, output_size=1, learning_rate=0.001, dropout=0.2): # , hidden_size=128\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # LSTM for time-series data (stock returns)\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            # num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        # FC layer for final prediction\n",
    "        self.fc_final = nn.Linear(hidden_size, 12)\n",
    "\n",
    "    def forward(self, ts_batch): # ts_batch (64, 1653, 10), static_data (64, 1653, 44)\n",
    "        # Time-Series Data\n",
    "        # Reshape dynamic data for LSTM (requires time-step as 2nd dimension)\n",
    "        batch_size, num_stocks, sequence_length = ts_batch.shape[0], ts_batch.shape[1], ts_batch.shape[2]\n",
    "        ts_batch_reshaped = ts_batch.view(batch_size * num_stocks, sequence_length)\n",
    "        #\n",
    "        ts_output_1, (hidden, cell)  = self.lstm(ts_batch_reshaped) # ts_batch_reshaped\n",
    "\n",
    "        ts_output = ts_output_1.view(batch_size, num_stocks, self.hidden_size)\n",
    "        #\n",
    "        # ts_output_2 = self.fc_lstm(ts_output)\n",
    "        #fc_final = nn.Linear(sequence_length, 1)\n",
    "        # prediction =   # (64, 1653, 10)\n",
    "\n",
    "        return self.fc_final(ts_output)#.squeeze(-1) # ts_output_2"
   ],
   "id": "ff4cbc246759fc20",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:29:37.584303Z",
     "start_time": "2025-05-07T19:29:37.582192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set sequence length (12 months)\n",
    "in_seq_length = 12\n",
    "out_seq_length = 12"
   ],
   "id": "4bb4ad974e9f4f73",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1 Month",
   "id": "d5318687c5affaa7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:29:38.445549Z",
     "start_time": "2025-05-07T19:29:37.606477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "importlib.reload(lstm_utils)\n",
    "\n",
    "# Set sequence length (e.g., 12 time points)\n",
    "X_train_1m, X_test_1m, y_train_1m, y_test_1m = lstm_utils.split_train_test(df_time_series, [], in_seq_length=12, out_seq_length=1)\n",
    "\n",
    "# Check the shapes of the training and test data\n",
    "print(\"Shape of X_train:\", X_train_1m.shape)\n",
    "print(\"Shape of y_train:\", y_train_1m.shape)\n",
    "print(\"Shape of X_test:\", X_test_1m.shape)\n",
    "print(\"Shape of y_test:\", y_test_1m.shape)"
   ],
   "id": "a758546911c3a7a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: torch.Size([227, 1653, 12])\n",
      "Shape of y_train: torch.Size([227, 1653, 1])\n",
      "Shape of X_test: torch.Size([59, 1653, 12])\n",
      "Shape of y_test: torch.Size([59, 1653, 1])\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6 Months",
   "id": "43a9fdefde6c005"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:45:03.228591Z",
     "start_time": "2025-05-07T19:45:02.288339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "importlib.reload(lstm_utils)\n",
    "\n",
    "# Set sequence length (e.g., 12 time points)\n",
    "X_train_6m, X_test_6m, y_train_6m, y_test_6m = lstm_utils.split_train_test(df_time_series, [], in_seq_length=12, out_seq_length=6)\n",
    "\n",
    "# Check the shapes of the training and test data\n",
    "print(\"Shape of X_train:\", X_train_6m.shape)\n",
    "print(\"Shape of y_train:\", y_train_6m.shape)\n",
    "print(\"Shape of X_test:\", X_test_6m.shape)\n",
    "print(\"Shape of y_test:\", y_test_6m.shape)"
   ],
   "id": "1696162ee878eb39",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: torch.Size([228, 1653, 12])\n",
      "Shape of y_train: torch.Size([228, 1653, 6])\n",
      "Shape of X_test: torch.Size([54, 1653, 12])\n",
      "Shape of y_test: torch.Size([54, 1653, 6])\n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:56:08.037093Z",
     "start_time": "2025-05-07T19:56:04.582807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "importlib.reload(lstm_utils)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model_6m = LSTMModel(input_size=12, output_size=12).to(device)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model_6m.parameters(), lr=0.001)\n",
    "\n",
    "# @TODO resume here (fix test output)\n",
    "model, y_train_pred, y_test_pred = lstm_utils.lstm_train_validate(model_6m, optimizer, X_train_6m, X_test_6m, y_train_6m, y_test_6m)"
   ],
   "id": "10ce63dabded6f1d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/herbishtini/anaconda3/envs/portfolio_optimisation/lib/python3.11/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([32, 1653, 6])) that is different to the input size (torch.Size([32, 1653, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (12) must match the size of tensor b (6) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[114], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(model_6m\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# @TODO resume here (fix test output)\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m model, y_train_pred, y_test_pred \u001B[38;5;241m=\u001B[39m lstm_utils\u001B[38;5;241m.\u001B[39mlstm_train_validate(model_6m, optimizer, X_train_6m, X_test_6m, y_train_6m, y_test_6m)\n",
      "File \u001B[0;32m~/Documents/UNI/Master Thesis/sustainability_portfolio_optimisation/utilities/lstm_utils.py:95\u001B[0m, in \u001B[0;36mlstm_train_validate\u001B[0;34m(model, optimizer, X_train, X_test, y_train, y_test)\u001B[0m\n\u001B[1;32m     92\u001B[0m y_train_pred_batch \u001B[38;5;241m=\u001B[39m model(X_batch)\n\u001B[1;32m     94\u001B[0m \u001B[38;5;66;03m# Compute loss\u001B[39;00m\n\u001B[0;32m---> 95\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(y_train_pred_batch, y_batch)\n\u001B[1;32m     96\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     98\u001B[0m \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/portfolio_optimisation/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/portfolio_optimisation/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/portfolio_optimisation/lib/python3.11/site-packages/torch/nn/modules/loss.py:538\u001B[0m, in \u001B[0;36mMSELoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 538\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mmse_loss(\u001B[38;5;28minput\u001B[39m, target, reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduction)\n",
      "File \u001B[0;32m~/anaconda3/envs/portfolio_optimisation/lib/python3.11/site-packages/torch/nn/functional.py:3383\u001B[0m, in \u001B[0;36mmse_loss\u001B[0;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[1;32m   3380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3381\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 3383\u001B[0m expanded_input, expanded_target \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mbroadcast_tensors(\u001B[38;5;28minput\u001B[39m, target)\n\u001B[1;32m   3384\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39mmse_loss(expanded_input, expanded_target, _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction))\n",
      "File \u001B[0;32m~/anaconda3/envs/portfolio_optimisation/lib/python3.11/site-packages/torch/functional.py:77\u001B[0m, in \u001B[0;36mbroadcast_tensors\u001B[0;34m(*tensors)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function(tensors):\n\u001B[1;32m     76\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(broadcast_tensors, tensors, \u001B[38;5;241m*\u001B[39mtensors)\n\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _VF\u001B[38;5;241m.\u001B[39mbroadcast_tensors(tensors)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (12) must match the size of tensor b (6) at non-singleton dimension 2"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 12 Months",
   "id": "f32c4b7502d9dfb9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Split the data into training and testing sets",
   "id": "e6383e415b7539d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train-Test Split",
   "id": "6ce4ba4bb6d29a7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:47:05.607329Z",
     "start_time": "2025-05-07T19:47:04.430055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "importlib.reload(lstm_utils)\n",
    "\n",
    "# Set sequence length (e.g., 12 time points)\n",
    "X_train_12m, X_test_12m, y_train_12m, y_test_12m = lstm_utils.split_train_test(df_time_series, [], in_seq_length=12, out_seq_length=12)\n",
    "\n",
    "# Check the shapes of the training and test data\n",
    "print(\"Shape of X_train:\", X_train_12m.shape)\n",
    "print(\"Shape of y_train:\", y_train_12m.shape)\n",
    "print(\"Shape of X_test:\", X_test_12m.shape)\n",
    "print(\"Shape of y_test:\", y_test_12m.shape)"
   ],
   "id": "15a242ca157c28d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: torch.Size([227, 1653, 12])\n",
      "Shape of y_train: torch.Size([227, 1653, 12])\n",
      "Shape of X_test: torch.Size([48, 1653, 12])\n",
      "Shape of y_test: torch.Size([48, 1653, 12])\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:47:06.995510Z",
     "start_time": "2025-05-07T19:47:06.988366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model, Loss, Optimizer\n",
    "model_12m = LSTMModel(input_size=in_seq_length, output_size=out_seq_length).to(device)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model_12m.parameters(), lr=0.001)"
   ],
   "id": "44c1702b803110f6",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:48:14.747870Z",
     "start_time": "2025-05-07T19:47:20.753314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "importlib.reload(lstm_utils)\n",
    "#\n",
    "model, y_train_pred, y_test_pred = lstm_utils.lstm_train_validate(model_12m, optimizer, X_train_12m, X_test_12m, y_train_12m, y_test_12m)"
   ],
   "id": "22cdddc240803495",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 31.5999, Train RMSE: 5.9694, Test RMSE: 0.2869. \n",
      "Model training complete and saved.\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:29:43.690905Z",
     "start_time": "2025-04-13T23:05:59.984302Z"
    }
   },
   "cell_type": "code",
   "source": "last_month = X_test_12m[[len(X_test_12m) - 1]]",
   "id": "113b405d49a85b6e",
   "outputs": [],
   "execution_count": 298
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:29:43.690942Z",
     "start_time": "2025-04-13T23:09:01.512441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    last_month_pred = model(last_month)"
   ],
   "id": "d767d07c58c6d7b5",
   "outputs": [],
   "execution_count": 303
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:29:43.690975Z",
     "start_time": "2025-04-13T23:21:10.941085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Actual values\n",
    "y_test_12m = y_test_12m[len(y_test_12m) - 1][:, -1]\n",
    "# Predicted\n",
    "y_test_pred_12m = last_month_pred[0][:, -1]"
   ],
   "id": "765bc49dac33ad2d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0553,  0.0107,  0.0534,  ...,  0.0438, -0.0076, -0.0185])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 312
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
